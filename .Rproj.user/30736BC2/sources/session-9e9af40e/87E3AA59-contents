---
title             : "Quantifying the Impact of Current NBA Coaches"
shorttitle        : "Coaching Impact"

author: 
  - name          : "Shane Faberman"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "faberma@unc.edu"

affiliation:
  - id            : "1"
    institution   : "University of North Carolina at Chapel Hill | Sports Analytics Intelligence Lab"

abstract: |
  In the NBA, coaches play a crucial role in game strategy, player development, and managing team dynamics. However, quantifying coaching impact remains a challenge, as it is difficult to isolate a coach’s influence from that of their players. Unlike the plethora of statistics available for evaluating players, coaching performance is assessed far more subjectively. This study introduces a new metric, Box Plus-Minus (BPM) Over Expected (BOE), that evaluates coaches based on how their players perform relative to expectations, aiming to identify those who consistently maximize their players' potential. To calculate BOE, Expected BPM (EBPM) was first computed for each player-season. Let a player's age n season represent the season when they were n years old. EBPM was derived by adjusting a player’s BPM from their age n-1 season based on average aging trends for qualifying players and the deviation from the league-average qualifying player at age n-1. BOE was then calculated as the difference between a player’s actual BPM in their age n season and their EBPM. According to BOE, media coach rankings tend to overrate championship-winning coaches. This is likely due to underrating the effect of superstar players such as Lebron James, Stephen Curry, and Nikola Jokić. Championships and regular-season win-loss records depend on many factors not under a coach's control. BOE provides a more nuanced assessment of a coach's system's impact by shifting the focus toward individual player performance relative to expectation. It serves as a valuable tool, in conjunction with other factors, for evaluating coaching effectiveness.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "NBA, sports analytics, basketball, coaching"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : 
  papaja::apa6_pdf:
     extra_dependencies: ["float"]
  
header-includes:
  - \usepackage{longtable}
  - \usepackage{float}
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
library(tidyverse)
library(glue)
library(rvest)
library(hoopR)
library(nflplotR)
library(nbaplotR)
library(ggnewscale)
library(ggtext)
library(ggtext)
library(gt)
library(grid)
library(webshot2)
library(knitr)
library(stats)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo = F, fig.pos= "H", out.extra='')
```

# Introduction

In February of 2024, four-time NBA Champion Head Coach Steve Kerr of the Golden State Warriors signed a two-year, \$35 million contract extension [@kerr]. In the 2024-25 season, Kerr's \$17.5 million salary was larger than every Warrior except two-time MVP Stephen Curry and a pair of likely Hall of Famers in Jimmy Butler and Draymond Green. Did Kerr deserve this record-breaking extension? How sure can we be that he is one of the NBA's top coaches? There are countless measurements of player production used to assess performance, rank players, and determine contract worth, such as box score counting stats, all-in-one impact metrics, on/off net-rating swings, etc. However, there are not regularly cited metrics used to determine a coach's impact on their team. While in practice, we do not care if a coach is "worth" their salary because it is not counted against the team's salary cap, finding a way to quantify the impact of an NBA coach would be extremely useful for fans, media members, and NBA front offices.

Quantifying a coach's impact on a team in any sport is an extremely difficult task. No number can account for the management of personalities, leadership, ability to establish a championship-winning culture, design of practices, etc. However, it is especially difficult to do in the NBA because there are not many aspects of the game that completely depend on a coach's decision. In the NFL, for example, a team's decision to go for 4th downs can be entirely traced back to a coach's decision and the win probability won/lost as a result of that decision can be measured.

One aspect of a game that an NBA coach has total control over is when they call their timeouts. Research supports that timeouts can slow down opponent runs [@timeouts], which means that a coach who effectively uses their timeouts can add a small amount of win probability on the margins of a game. Another way to assess a coach is how well a team performs in their possessions after a timeout (ATOs). The paper finds a fairly positive correlation between a team's win percentage and effectiveness on these ATOs. However, it could be argued that ATO performance is equally dependent on player quality. A good play still needs to be executed, and better players tend to execute these plays more effectively than worse ones.

All-in-one metrics are becoming an increasingly common way to measure player impact. These metrics include BPM (Box Plus-Minus), EPM (Estimated Plus-Minus), DARKO/DPM (Daily Plus-Minus), LEBRON (Luck-adjusted player Estimate using a Box prior Regularized ON-OFF), RAPTOR (Robust Algorithm (using) Player Tracking (and) On/Off Ratings), RAPM (Regularized Adjusted Plus-Minus), Net Points, AuPM (Augmented Box Plus-Minus), and many more. These metrics estimate a player's impact on a team, often over a fixed number of possessions. In simple terms, Plus-Minus metrics take how much a team outscores its opponent when player X is on the floor and adjusts the number based on teammate and opponent quality, garbage time, home-court advantage, and numerous other factors. RAPM, for example, can be calculated following the process explained in this cited paper: [@rapm].

One of the few metrics that measures coaching performance is Coach RAPM created by Jeremias Engelmann [@xrapm]. This metric treats a coach as the 6th player on the floor and projects a coach's impact adjusted for player quality. Coach RAPM tries to estimate a coach's on-court value. This is a useful metric and one that I will refer to later in this paper to help compare results. However, one of the most important functions of a coach is player development. The best coaches should, in theory, get the most out of their players. If a player plays under a great coach and develops with the help of said coach, that coach might be dinged in the player quality adjustment because the their players might be consistently good due to the coach's abilities to put their players in the right situation.

Instead of viewing a coach as another player, I intend to quantify coaching impact by looking at which coaches get the most out of their players. As stated earlier, in theory, a good coach should get more out of their players than a bad coach. This paper attempts to create a metric that measures which current NBA coaches get the most out of their players.

# Methods

## Data

To answer this question in terms of current NBA coaches, this study is restricted to the last 10 seasons **(2015-16 to 2024-25)**. The rationale for this decision is similar to analyzing players --- a logical front office is not going to sign a player for a season they had many years ago, nor should they hire a coach for a good run they had in a different era of basketball. Thanks in large part to Steph Curry, James Harden, and the three-point revolution, the NBA is far different than it was just fifteen years ago. With the goal of this paper being to try to identify the best current coaches in the league, it makes sense to restrict coaching performance to just this most recent era.

Data for this study was scraped from Basketball Reference [@bballref]. Only seasons in which a coach started and finished the seasons were used in the calculation for each coach, so for example, since Tom Thibodeau was fired from the Minnesota Timberwolves in January 2019, the 2018-19 season is not part of his metric. This decision was made because of the difficulty in scraping thousands of single-game advanced box scores and calculating overall advanced stats for players that way. Instead, advanced stats were scraped from Basketball Reference's advanced stats page for each of the seasons in the study. Players were filtered to those who played at least 1000 minutes in season $N$ and 500 minutes in season $N-1$. Since this study relies on the use of BPM in both season $N$ and season $N-1$, it was important for the sample size of minutes played to be large enough. On average, around seven to nine players per team play 1000 minutes each year, which is also the size of most teams' playoff rotations. The 500 minutes in the prior season was implemented to prevent a season-ending injury or too small of a sample size from skewing results. In addition, the smaller cut-off for the year prior was also used to make sure players who had a large jump in their role, say from an 11th man to a 7th man, were not excluded due to not playing 1000 minutes the season prior.

Rookies are an exception to this, as they could not have played 500 minutes in the season prior. Another slight exception is players who played on multiple teams in a season. To qualify, a player still had to meet the 1000 minutes in season $N$ and 500 minutes in season $N-1$ baseline. From there, each team he was on that season was treated as a different observation, and the baseline for including those was 250 minutes in season $N$. For example, if two players who were traded at the trade deadline played 1200 minutes total, but A played 700 minutes with team one and 500 with team two, and B played 1100 minutes with team one and 100 minutes with team two, then player A has separate entries for season $N$ with team one and team two, while player B just has an entry for his time with team one.

## Formula

To determine which coaches get the most out of their players, a measure of expectation of performance can be compared to actual performance. I used Basketball Reference's BPM as the measure of performance and computed an expected BPM **(EBPM)**. Then, BPM Over Expectation **(BOE)** could be calculated by subtracting EBPM from BPM.

**EBPM**: My definition of EBPM takes the input of a player, considers their BPM from the prior season, and adjusts that number based on the change in the player's age. The first component of the formula handles the age adjustment. I started with the basic percent change formula, where $\mu_n$ represents the average BPM for a qualifying player at age $n$ and $\mu_{n-1}$ follows an analogous definition for age $n-1$:

$$
X_n = \frac{\mu_n - \mu_{n-1}}{\left| \mu_{n-1} \right|}
$$

However, there are a couple of problems with $X_n$. The first problem is easy to visualize with this plot:

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/age_curve.png}
\end{center}
```

```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 1}\\end{center}")
```

The answer to this question lies in the fact that better players play longer than worse players. There is a higher concentration of star players in their mid-30s than in their mid-20s. While it looks like a player's BPM largely stays the same for the rest of their career once they reach 30, that obviously is not the case. Players decline, sometimes pretty rapidly, and eventually, they are not in the NBA anymore, whether by choice or not. The solution for this problem is that when computing $X_n$, we only consider the average BPM for qualifying players who played in both their age $n$ and age $n-1$ seasons. This filters out retired players from this calculation and handles rookies coming into the NBA at different ages. We can call this adjusted value $X_n'$. By looking at the figure below, we can visualize the second problem.

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/loess.png}
\end{center}
```

```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 2}\\end{center}")
```

The reason age 23 is such an outlier is because $\mu_{22}'$ is very close to 0. This outlier would lead to some very inaccurate EBPM values for 23-year-old players. To fix this problem and obtain a more accurate, smooth age curve, we can implement a loess model (a locally weighted regression model) for age as shown in the visual. We can call this modified value $X_n''$.

It is not enough to simply multiply a player's BPM from their prior season ($BPM_{n-1}$) by this $X_n''$ value to obtain their EBPM. For starters, this would imply that no matter what age a player is, if they had a BPM of 0.0 in the prior season, their EBPM would be 0. Further, when the sign of $X_n''$ differs from the sign of $BPM_{n-1}$, this formula would cause wild inaccuracies. For example, if the average player gets twice as good from age $n-1$ to age $n$, and a player $P$ has a BPM of -2.0 at age $n-1$, their EBPM would be -4.0, which is not what we actually expect. This same problem happens when the sign of $X_n''$ is negative and the sign of $BPM_{n-1}$ is positive. The distribution of BPM of qualifying players is centered at around 0.0, so these problems are prevalent. 

In addition, $X_n''$ measures the change of average BPM values. If a player's BPM is far away from the average, it does not make sense to treat them the same as an average player. For example, we can take three 22-year-old players: JaKarr Sampson in 2016, Luka Dončić in 2022, and Malik Beasley in 2019. These players had a BPM of -6.7, 8.2, and 0.1, respectively. A superstar like Dončić does not have much room to improve his impact, compared to league-average players such as Beasley. Even though the jump from age 22 to 23 is, on average, a significant one, it does not make sense to apply the same jump to both players. In the case of Sampson, we do not expect players near the bottom of the league in terms of impact to be in the NBA for as long as an average player, so they also will probably not follow the same development curve. To quantify this, we can use this formula, where $BPM_{n-1}$ represents a player's BPM for a player $P$ at age $n-1$, $\mu_{n-1}$ is the average BPM for players at age $n-1$, and ${\sigma_{n-1}}$ is the standard deviation of BPM for players at age $n-1$: 

$$
Y(P) = \frac{1}{ 1 + \frac{\left| BPM_{n-1} - \mu_{n-1} \right|}{\sigma_{n-1}} }
$$

By using the absolute value of the z-score, players who have a BPM closer to the mean are affected the most by the change. Putting the two terms together, we have, for any player $P$ at age $n$: 
$$
EBPM_n(P) = BPM_{n-1} + X_n''\cdot Y(P)
$$

**Rookies**: Obviously, the above formula is useless for rookies. To create an EBPM metric for rookies, I first scraped every draft since 2003. This cutoff was chosen because the 2003 draft is the earliest one that still includes at least one active NBA player.

A "rookie" was defined as a player's first season where they played at least 36 minutes. This cutoff is admittedly a bit arbitrary, but it was important to strike a balance between the true definition of being a rookie (the season where a player plays their first minute) and extremely small sample sizes of minutes played skewing BPM numbers. I chose 36 minutes because it is a "full game" worth of minutes (this comes from the idea of per-36-minute stats). This way, I define rookies as when they play their first full games worth of minutes, rather than their first minutes in general. This does not get rid of all of the small samples, but it gets rid of a fair amount of them. If a player did not play at least 36 minutes in each of their first three seasons, they were given a value of NA, as that made them an outlier.

It is not enough to simply take the mean of the BPM for every draft slot to define EBPM for rookies. When looking at second-round picks and undrafted free agents (UDFAs), there is a fair amount of survivorship bias. First-round picks, especially lottery picks, will almost always have on-court opportunities early in their careers. For the most part, only the best second-round picks and UDFAs will ever even be given an opportunity, as many of these players who are not good enough will not make the roster.

Intuitively, the first overall pick should be expected to be better than the second overall pick, the second overall pick should be expected to be better than the third overall pick, and so on. This is the rationale for applying a logarithmic model (a strictly decreasing function) on the mean BPM by draft selection (UDFAs were given a draft selection of 61). This is visualized below.

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/rookie.png}
\end{center}
```

```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 3}\\end{center}")
```

Each rookie was given an EBPM value according to this model. Therefore, every first-overall pick had the same EBPM, every second-overall pick had the same EBPM, etc.

**Other Metrics**: No matter how good a coach is, they can only elevate lesser talent for so long. A player very likely cannot outperform expectations for their entire tenure with a coach, but that should not mean a coach does not get credit for developing that player. In this lens, we can look at the immediate impact a coach has on a player with the following metrics:

$BOE_{before}$: For any coach, this metric measures the mean BOE for players who played for the coach in any season $N$ and a different coach in season $N-1$. This also includes rookies. The intuition for this metric is that the best coaches should provide an immediate impact, and players should, in theory, overperform relative to expectations in their first year with the coach.

$BOE_{after}$: For any coach, this metric measures the mean BOE for players who played for the coach in any season $N$ and a different coach in season $N+1$. The intuition for this metric is that when players stop playing for one of the best coaches in the league, they should, in theory, underperform in their first year without the coach.

\newpage

## Software and Packages used

`r cite_r("r-references.bib")`

\newpage

# Results

## Validating BOE

Every case in which a team had multiple coaches in a season was given a label in the coach column of "Multiple Coaches". While there are rare cases where this does not mean a coach was fired (such as the 2024-25 Spurs), the large majority of these were cases in which the team fired their coach mid-season.

**Hypothesis 1:** *Since teams who fire their head coach mid-season tend to be underperforming, the mean BOE for "Multiple Coaches" should rank near the bottom of every qualifying coach.*

"Multiple Coaches" ranks 58th/64 qualifying coaches. The only coaches behind him are Frank Vogel, Nate Bjorkgren, Jim Boylen, George Karl, Byron Scott, and Brian Keefe. Bjorkgren only lasted one season with the Pacers. Boylen only lasted two seasons with the Bulls. Karl and Scott both were fired after the 2015-16 season (the first season in the sample) and have not coached since. Keefe's only season as a full-time head coach is currently the 18-64 Wizards. Vogel won a championship with the Lakers in 2020, but he has been fired from four different jobs since 2016, the most of any coach since then. This hypothesis seems to hold.

**Hypothesis 2:** *There are 21 coaches who have had multiple head coaching jobs since 2015-16. Mean BOE for each coach should be independent of job. There should be no significant difference between mean BOE for a coach in their first job in the sample and the mean BOE for a coach in their second job in the sample.*

To test this, I performed a paired t-test. The null hypothesis was that the difference between the mean BOE for a coach in their first job in the sample is equal to the mean BOE for a coach in their second job in the sample. The alternative hypothesis was that the difference between these two values is not equal to 0. The test returned a p-value of 0.3514 (95% Confidence Interval: [-0.1539, 0.4135]), meaning there is no significant evidence that the true difference between these means is not 0. This hypothesis seems to hold.

**Hypothesis 3:** *Since the NBA's Coach of the Year (COTY) award is given to one of the league's most impressive coaching performances of the season, the winners should also have an impressive mean BOE value for that season.*

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/coty_tbl_poster.png}
\end{center}
```
```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 4}\\end{center}")
```

Per Figure 4, seven of the ten COTY winners finished top five in single-season mean BOE, and six finished top three. Kerr's Warriors, the only coach that did not finish in the top ten, were coming off a league-best 67 wins in 2014-15, so it is not all that surprising that the Warriors were not much better than expected. Monty Williams' Suns and Dwane Casey's Raptors were both coming off of 50-win seasons, with the Suns losing in the NBA Finals as well. These teams had high, championship-level expectations coming into these seasons, so it makes sense why the mean BOE numbers for Williams and Casey are not as high. The COTY award is extremely narrative-driven, so the award is not always given to the coach who performed the "best". Not to say that the COTY award should be given to the coach who ranks the highest in mean BOE during that season, but the award winner is not necessarily the best coach from that year. This hypothesis seems to hold.

\newpage

## Current Coach Analysis

One way to look at mean BOE is by comparing it to consensus media coach rankings. Unlike player rankings, coach rankings are published by far fewer media outlets and are not updated during the season, so the consensus used is from the 2024-25 preseason ([@cbs], [@newarena], [@msn], [@footboom], [@tps], [@ripcity], [@sircharles]).

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/boe_graph.png}
\end{center}
```
```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 5}\\end{center}")
```
```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Note: The coach directly under Darko Rajakovic is Billy Donovan.}\\end{center}")
```
In the bottom right quadrant, we find the coaches who won seven of the eight qualifying championships in the last decade (this paper was written before the 2025 NBA Finals, and Frank Vogel, head coach of the 2020 Lakers does not currently have a head coaching job[^1]). They are joined by Jason Kidd, Chris Finch, Gregg Popovich, and Tom Thibodeau. It is easy to hypothesize why Popovich, despite arguably being the greatest coach of all time, is classified as overrated by mean BOE. The Spurs have not won more than 50 games since the 2016-17 season and have not made the playoffs in over a half-decade. Since this study only involves the last ten seasons, Popovich's numbers unsurprisingly are unimpressive.

[^1]: As discussed earlier, Vogel does not grade out well in mean BOE. Vogel's mean BOE is the 6th worst of all coaches since 2015-16. He would've likely also been in the bottom right quadrant if he had a job, as he's widely viewed as an above-average coach.

The common denominator for these other highly rated coaches performing worse than expected is that they have had their fair share of superstar players.

-   Steve Kerr's first season as a head coach was Steph Curry's first MVP campaign, and his prime has lasted Kerr's entire tenure. In addition, Kerr had three seasons of prime Kevin Durant, and future Hall of Famers Draymond Green and Klay Thompson also played their prime years under Kerr.

-   Three-time MVP Nikola Jokić's rookie season was Michael Malone's first season as the head coach of the Nuggets.

-   Tyronn Lue has gotten the opportunity to coach Lebron James in Cleveland and Kawhi Leonard in Los Angeles, who have won a combined six NBA Finals MVPs.

-   Nick Nurse only coached Kawhi Leonard for one season, but Kawhi's 30.4 points per game with a True Shooting Percentage of 61.9% is widely regarded as one of the greatest individual playoff runs of all time. Nurse only coached Joel Embiid for (effectively) one season, but Embiid scored 34.7 points in just 33.6 minutes per game with a 64.4% TS%.

-   Jason Kidd has reached two Western Conference Finals and one NBA Finals with the Dallas Mavericks, but that probably has a lot to do with Luka Dončić, who is second to only Michael Jordan in career playoff points per game.

-   Chris Finch helped lead the Timberwolves to the Western Conference Finals in the 2023-24 season. While neither is as good as the players previously mentioned, Anthony Edwards (top ten all-time in playoff points per game) and Rudy Gobert (Four-time NBA Defensive Player of the Year) were the drivers of the run.

-   Though Tom Thibodeau has yet to have a deep playoff run with the Knicks, New York has enjoyed their most successful era of basketball since the 1990s under him. Thibodeau certainly deserves some credit, especially for the 2020-21 season, but the signing of Jalen Brunson took the Knicks from a bad offensive team to a great one overnight. Thibodeau tried to turn another cursed franchise around in Minnesota, but he only had success when Jimmy Butler was healthy.

-   Joe Mazzulla is an interesting case. The Celtics under Mazzulla have embraced the three-pointer like no team in NBA history, and it has led to great results. In addition, they consistently employ some of the most unique defensive schemes in the NBA. Mazzulla has clearly pushed the right buttons to help push his team over the top. However, Mazzulla took over for a team coming off a trip to the 2022 NBA Finals. He was given a team with Jayson Tatum, Jaylen Brown, and Derrick White, and Boston effectively flipped Marcus Smart for two All-Stars in Kristaps Porziņģis and Jrue Holiday before the 2023-24 season. Did the Celtics break three-pointer records because of Mazzulla or because Tatum, Brown, White, Porziņģis, Holiday, Al Horford, Payton Pritchard, and Sam Hauser might be the best collection of shooting talent on a team ever? Are the Celtics' defensive schemes so effective because of Mazzulla, or is it due to 6'4" Jrue Holiday's ability to guard bigger players better than almost anyone in the NBA, combined with a starting five made up of individually good-to-elite defenders? It is likely due to a combination of both, but it is hard to say exactly how much credit should be given to Mazzulla. Despite this, Mazzulla is very likely a better coach than his mean BOE suggests, as he has only been coaching three seasons and the sample size is very small.

This is not to say these coaches are all bad at their jobs. It is absolutely unfair to hold these superstars against their coaches. However, though it seems impossible, we might understate the impact of superstars in the NBA. More than any other league, the NBA is a superstar-driven league because there are just five players on the court at once. Upsets are much rarer in the NBA playoffs than in leagues such as the MLB and NFL, and the best teams win the championship at an extremely high rate. All this is to say that the impact of coaching might matter less than fans think with regard to winning a championship. Fans are quick to want to fire their coach when a season goes poorly, but they might be placing too much blame on their coach and not enough on the quality of the roster. Likewise, ranking coaches predominately on the success of their teams likely leads to inaccurate rankings.

As discussed earlier, coaches can only elevate the talent on their roster for so long. Instead of looking at overall mean BOE numbers, looking at the immediate impact of coaches using $BOE_{before}$ and $BOE_{after}$ gives a different perspective.

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/before_after.png}
\end{center}
```
```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 6}\\end{center}")
```

Despite having one of the largest samples, Erik Spoelstra dominates the $BOE_{before}$ metric. Players consistently improve when they first get to Miami, and this is a major reason that Spoelstra is the consensus best coach in the NBA. Additionally, players also tend to underperform directly after leaving Miami, strengthening the argument that Spoelstra's system elevates his players.

As previously mentioned, championship-winning coaches of the last decade do not grade out as well as expected in mean BOE. The one championship-winning coach of the last decade who had a positive residual in Figure 5 was Mike Budenholzer, who is one of the top coaches in both $BOE_{before}$ and $BOE_{after}$. Budenholzer was fired after his top-seeded Bucks lost in five games to the eight-seeded Miami Heat in the 2023 playoffs, but Giannis Antetokounmpo missed two games of that series. In the last two seasons, the Bucks have not been nearly as good. This is more likely due to the aging of their core players than their coaching, but it is also true that Budenholzer likely was not the problem and was wrongfully scapegoated.

Though Ime Udoka only has three seasons of experience, he deserves to be in the elite coach conversations. In addition to his elite overall mean BOE, $BOE_{before}$, and $BOE_{after}$ numbers, Udoka also ranks first among active coaches in Coach RAPM.

It is not hard to tell from Figure 5 that BOE and average media consensus ranking are not very correlated. Fitting the linear model shown previously gives an $R^2$ value of very close to 0 and a p-value of 0.4307, suggesting that media rankings are not statistically significant in explaining mean BOE. A simple metric that mean BOE is much more correlated with is a coach's average wins over their team's preseason projected wins, measured by Vegas Over/Under totals.


```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/ou.png}
\end{center}
```

```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 7}\\end{center}")
```
```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Note: The coach directly under Erik Spoelstra is Rick Carlisle.}\\end{center}")
```

Even though many of these coaches have only a few seasons under their belt and Wins Over Preseason O/U can be heavily influenced by outliers, fitting a linear model gives an $R^2$ value of  0.3574 and a p-value of 0.0013, meaning there is strong evidence that Wins Over Preseason O/U is statistically significant in explaining mean BOE. Intuitively, this makes sense, as if a team outperforms expectations, its players should also do the same.

More interesting, though, is comparing coaches' RAPM and mean BOE numbers. Since RAPM is the only current true coaching metric available, it is helpful to use it as a comparison to see which coaches are liked by both metrics as opposed to one more than another.

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/boe_rapm.png}
\end{center}
```

```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 8}\\end{center}")
```

```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Note: The coach directly under J.B. Bickerstaff is Quin Synder and the coach directly under Mike Budenholzer is Jamahl Mosley.}\\end{center}")
```

Fitting a linear model gives an $R^2$ of 0.1689 and a p-value of 0.0370, meaning there is evidence that these two metrics are correlated and help to explain each other. There are some striking differences for some coaches though, particularly a few in the top left quadrant. For Thibodeau, Popovich, and Doc Rivers, it makes sense that RAPM would like them more than mean BOE, as RAPM covers their entire coaching careers, while BOE is just the last decade. Those three coaches all had lots of success before 2015-16, which could explain why their RAPM numbers are higher. Jordi Fernandez and Will Hardy are both inexperienced coaches, and even though they have gotten more out of their talent than expected, their overall records are unimpressive. This could be why they grade out better in mean BOE than RAPM.

By using these metrics in conjunction with one another, we can start to build an analytical profile for NBA coaches. The table below provides a summary of all 30 coaches who started the 2024-25 season with a job.

```{=latex}
\begin{center}
\includegraphics[width=1\textwidth]{/Users/shanefaberman/Downloads/summary_poster.png}
\end{center}
```
```{r, results='asis', echo=FALSE}
cat("\\begin{center}\\textit{Figure 9}\\end{center}")
```

This summary can be used in conjunction with more traditional methods of coaching evaluation to paint a more complete picture of coaching impact. BOE only attempts to answer the question of what coaches get the most out of their players, which is different than RAPM. Completing this analytical summary and including other existing metrics such as team points per possession on ATOs and even creating new metrics can hopefully give complete profiles on coaches, and we will be able to assess a coach's strengths and weaknesses more objectively. It also allows us to criticize (and praise) a team's coaching decisions with more evidence to back up our claims.

For example, Taylor Jenkins grades out very well in the metrics presented in the table above. In addition, even though his BOE percentile is a very impressive 90.3, since he was fired before the end of the 2024-25 season, it does not factor in numbers from this year. Before his firing, Jenkins had the highest mean BOE of any coach for 2024-25, narrowly edging out J.B. Bickerstaff and Kenny Atkinson. His mean BOE and $BOE_{before}$ are understating his performance as the head coach for Memphis. While we may never know what happened behind the scenes, on the surface it is an extremely puzzling move.

The NBA world was shocked again a few weeks later when Michael Malone was fired three games before the playoffs. Malone was just two seasons removed from winning the NBA Finals. The logic for firing a coach with only a week left before the playoffs is very shaky, as there is no time to implement any new schemes, lineups, or plays. However, if this had happened after the playoffs, the decision would have been defensible. Malone grades out as an average coach in BOE, and prior to his firing Malone ranked 21st out of 27 coaches for the 2024-25 season (Kings, Spurs, and Grizzlies had multiple coaches). As mentioned earlier, much of Malone's success is likely in part because he was fortunate enough to coach Nikola Jokić for his entire career. He is a solid coach but is likely not an elite one.

# Discussion

Various factors must be taken into account when discussing this study's results. These are also avenues for potential further research.

-   BPM is rather outdated compared to other all-in-one impact metrics. BPM is an impact metric computed strictly from the box score. This means that, unlike other impact metrics, it does not include tracking or play-by-play data in its calculation [@bpm]. While still useful, BPM is not as reliable as newer impact metrics such as EPM. However, BPM still has value and was used because it is readily available, free, and data goes back over 50 years.
-   Mean BOE is simply a regular season calculation. The goal of an NBA team is always to win a championship, and you cannot win a championship without playoff success. However, with the current formula, playoff mean BOE is not feasible to calculate. Only a handful of players on a handful of teams play enough minutes in the playoffs each year to have a steady playoff BPM number. Due to this, it is hard to make claims on who the definitive best coaches in the league are solely based on mean BOE. Even though we can hypothesize that highly rated coaches such as Steve Kerr and Michael Malone are overrated due to the greatness of all-time great players in Steph Curry and Nikola Jokić, we can only leave it at that. Playoff performance is a necessary factor to consider when ranking players, and the same holds for coaches.
-   Potential 2024-25 All-Rookie F/C Kyle Filipowski, Utah's 32nd overall pick, started 27 games his rookie season. He posted a BPM of -1.0, giving him a BOE of 2.6. BOE gives this credit to Will Hardy. However, it was the front office who drafted Filipowski. How much credit should CEO of basketball operations Danny Ainge and the front office get for drafting Filipowski, and how much credit should Hardy get for developing, playing, and trusting him? This question does not have an easy answer, but it is unlikely that Hardy deserves 100% of the credit, which BOE would imply.
-   As previously stated, if a coach was fired mid-season, that season is not included in their BOE due to calculation difficulties. However, as explored earlier, these "Multiple Coach" seasons have a poor BOE, as teams who fire their coaches mid-season often are wildly underperforming. This means that for coaches such as Tom Thibodeau, Tyronn Lue, and Jason Kidd, their mean BOE might be overstated (Although all three of them performed lower in mean BOE than expected based on their media rankings).
-   The tenure of a coach with a team is often rather short. It is rare to see a coach such as Spoelstra, Popovich, and Kerr spend over a decade with the same team. Due to this, the sample size for many coaches is lacking. One outlier season could skew the results for a coach.
-   The formula for EBPM is very simple. Only a player's BPM and age are considered, and having a more complex function with more inputs might lead to more accurate predictions.

The results of this study should not be taken as the literal ranking of the best coaches in the NBA. This study only provides a potential answer to one question: "Which coaches get the most out of their players?" However, these results are still very useful because there is great value in determining which coaches help players outperform expectations, as this is a major aspect of a coach's job. Though certain people may have a different opinion on where it ranks in a coach's most important roles, it is undoubtedly something we should care about when evaluating a coach. In addition, when building models to predict player improvement for an upcoming season, whether at the front office, media, or fan level, a stat such as mean BOE can be used as a valuable input, as the quality of coaching is not a factor that a player can control, but it almost certainly has some effect on player improvement and performance.

Successfully quantifying coaching impact can greatly help front offices make critical decisions. Using a potential analytical profile such as the one shown previously can help front offices make decisions on whether to fire a coach, extend their contract, and help better evaluate head coaching candidates with previous experience. BOE is just a start, there are many avenues to furthering our knowledge of coaching impact. With how competitive the NBA is, any marginal edge a team can gain is valuable. If they have not started already, teams should look into this area of basketball analytics research.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
